# -*- coding: utf-8 -*-
"""bitirme2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12L51PEBgpSbsj0RbgUzX4tj762H8TTvh
"""

# Commented out IPython magic to ensure Python compatibility.
# import torch
# from torch.autograd import Variable
import pickle
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, precision_recall_curve, f1_score, fbeta_score, accuracy_score
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
from sklearn.metrics import (accuracy_score, log_loss, classification_report)

# from google.colab import drive
# drive.mount('/content/drive')

# data = pd.read_csv("/content/drive/My Drive/dataset/diabetes_binary_5050split_health_indicators_BRFSS2015.csv")

data = pd.read_csv("./diabetes_binary_5050split_health_indicators_BRFSS2015.csv")

data.shape

data.info()

data.describe()

data.isna().sum()

data.columns

sns.boxplot(data=data['BMI'], orient="h")

corr_matrix = data.corr()
plt.figure(figsize=(30, 15))
sns.heatmap(corr_matrix, annot=True)
plt.show()

corr_with_class = data.drop('Diabetes_binary', axis=1).corrwith(data.Diabetes_binary).sort_values()
corr_with_class.plot(kind='barh', figsize=(10, 7))

"""#Outliers analysis

detection
"""

# Q1 = data.quantile(0.25)
# Q3 = data.quantile(0.75)
# IQR = Q3 - Q1
# print(IQR)

"""removal of outliers"""

# data1 = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]
# data1.shape

"""# Dropping Duplicates"""

# dropping duplicate values
data.drop_duplicates(keep=False, inplace=True)

"""# Normalization"""

# feature_names = data.columns
# feature_names = feature_names.values
# feature_names = np.delete(feature_names, 0)
# feature_names
#
# type(feature_names)

# for feature_name in feature_names:
#      data[feature_name] = (data[feature_name] - data[feature_name].mean()) / data[feature_name].std()
data

data.describe()

corr_with_class = data.drop('Diabetes_binary', axis=1).corrwith(data.Diabetes_binary).sort_values()
corr_with_class.plot(kind='barh', figsize=(10, 7))

"""<h1>Modelling</h1>

# logistic reggession
"""
#
# cols = list(data.columns)
# cols.remove("Diabetes_binary")
# sampled, target = SMOTE().fit_resample(data[cols], data["Diabetes_binary"])
# X_train, X_test, Y_train, Y_test = train_test_split(sampled[cols],
#                                                     target,
#                                                     test_size=0.1,
#                                                     shuffle=True)
#
# print("Train Feature Size : ", len(X_train))
# print("Train Label Size : ", len(Y_train))
# print("Test Feature Size : ", len(X_test))
# print("Test Label Size : ", len(Y_test))
#
# data.shape
#
# logistic_model = LogisticRegression(solver='liblinear', random_state=0).fit(X_train, Y_train)
# predicted = logistic_model.predict(X_test)
# print("Train Accuracy : {:.2f} %".format(accuracy_score(logistic_model.predict(X_train), Y_train) * 100))
# print("Test Accuracy : {:.2f} %".format(accuracy_score(logistic_model.predict(X_test), Y_test) * 100))
#
# cm = confusion_matrix(Y_test, predicted)
# classes = ["0", "1"]
# disp = ConfusionMatrixDisplay(confusion_matrix=cm,
#                               display_labels=classes)
# fig, ax = plt.subplots(figsize=(10, 10))
# plt.title("Confusion Matrix")
# disp = disp.plot(ax=ax)
# plt.show()
#
# print(classification_report(predicted, Y_test))
# print("ACCURACY:", accuracy_score(Y_test, predicted))
# print("RECALL:", recall_score(Y_test, predicted, average="binary"))
#
# """# random forest
#
# """
#
# random_forest = RandomForestClassifier(n_estimators=20,
#                                        random_state=0).fit(X_train, Y_train)
# predicted = random_forest.predict(X_test)
# print("Train Accuracy : {:.2f} %".format(accuracy_score(random_forest.predict(X_train), Y_train) * 100))
# print("Test Accuracy : {:.2f} %".format(accuracy_score(random_forest.predict(X_test), Y_test) * 100))
#
# cm = confusion_matrix(Y_test, predicted)
# classes = ["No", "Yes"]
# disp = ConfusionMatrixDisplay(confusion_matrix=cm,
#                               display_labels=classes)
# fig, ax = plt.subplots(figsize=(10, 10))
# plt.title("Confusion Matrix")
# disp = disp.plot(ax=ax)
# plt.show()
#
# print(classification_report(predicted, Y_test))
# print("ACCURACY:", accuracy_score(Y_test, predicted))
# print("RECALL:", recall_score(Y_test, predicted, average="binary"))
#
# """# KNN"""
#
# knn = KNeighborsClassifier(n_neighbors=15)
# knn.fit(X_train, Y_train)
#
# y_pred = knn.predict(X_test)
#
# y_pred
#
# data['Diabetes_binary'].value_counts()
#
# print(confusion_matrix(Y_test, y_pred))
#
# cm = confusion_matrix(Y_test, y_pred)
# classes = ["No", "Yes"]
# disp = ConfusionMatrixDisplay(confusion_matrix=cm,
#                               display_labels=classes)
# fig, ax = plt.subplots(figsize=(10, 10))
# plt.title("Confusion Matrix")
# disp = disp.plot(ax=ax)
# plt.show()
#
# print(classification_report(Y_test, y_pred))
# print("ACCURACY:", accuracy_score(Y_test, predicted))
# print("RECALL:", recall_score(Y_test, predicted, average="binary"))
#
# # Running KNN for various values of n_neighbors and storing results
# # knn_r_acc = []
# # for i in range(1,17,1):
# #     knn = KNeighborsClassifier(n_neighbors=i)
# #     knn.fit(X_train,Y_train)
# #     test_score = knn.score(X_test,Y_test)
# #     train_score = knn.score(X_train,Y_train)
# #     knn_r_acc.append((i, test_score ,train_score))
# # df = pd.DataFrame(knn_r_acc, columns=['K','Test Score','Train Score'])
# # print(df)
#
# # k=15 found the for the best result
# # outcome:
# #      K  Test Score  Train Score
# # 0    1    0.659855     0.996558
# # 1    2    0.639277     0.824081
# # 2    3    0.680003     0.830821
# # 3    4    0.668674     0.791421
# # 4    5    0.691403     0.793447
# # 5    6    0.686241     0.777278
# # 6    7    0.701728     0.779357
# # 7    8    0.698717     0.768494
# # 8    9    0.709472     0.769373
# # 9   10    0.705815     0.761970
# # 10  11    0.712698     0.763601
# # 11  12    0.711981     0.758008
# # 12  13    0.716211     0.759281
# # 13  14    0.715781     0.754692
# # 14  15    0.720083     0.754782
# # 15  16    0.719796     0.751519
#
# """# Feature importance"""
#
# importance_rf = pd.DataFrame({
#     'feature': X_train.columns,
#     'score': random_forest.feature_importances_
# }).sort_values('score', ascending=False)
#
# importance_rf.head(10)
#
# # Income, Education, PhysActivity --> Negative correlation
# # GenHlth, HighBP, DiffWalk,BMI --> Positive correlation
# plt.figure(figsize=(15, 5))
# sns.set(font_scale=1)
# plt.title('Feature Importance according to random forest')
# ax = sns.barplot(data=importance_rf.head(10), x='score', y='feature')
# ax.set(xlabel='Score', ylabel='Feature')

corr_with_class.plot(kind='barh', figsize=(10, 7))

data.columns

data.shape

# I only want to leave these attributes: BMI, Age, Income, Education, HighBP and of course the class attribute

# these 11 attributes almost no effect on the modelling algorithms, so I dropped them
data.drop(['CholCheck'], axis=1, inplace=True)
data.drop(['Smoker'], axis=1, inplace=True)
data.drop(['Stroke'], axis=1, inplace=True)
data.drop(['HeartDiseaseorAttack'], axis=1, inplace=True)
data.drop(['PhysActivity'], axis=1, inplace=True)
data.drop(['Fruits'], axis=1, inplace=True)
data.drop(['Veggies'], axis=1, inplace=True)
data.drop(['Sex'], axis=1, inplace=True)
data.drop(['HvyAlcoholConsump'], axis=1, inplace=True)
data.drop(['AnyHealthcare'], axis=1, inplace=True)
data.drop(['NoDocbcCost'], axis=1, inplace=True)

# these 4 attributes are kinda effective, but I am choosing to drop them to increase user experience.
data.drop(['DiffWalk'], axis=1, inplace=True)
data.drop(['HighChol'], axis=1, inplace=True)
data.drop(['MentHlth'], axis=1, inplace=True)
# data.drop(['Education'], axis=1, inplace=True)
# data.drop(['HighBP'], axis=1, inplace=True)
data.drop(['PhysHlth'], axis=1, inplace=True)

# experimental
# this attribute is the most important attribute, but it does not make so much sense to use it since it already says the
# general health status
data.drop(['GenHlth'], axis=1, inplace=True)

# so many configurations has been tested according to the information that came
# from correlation and feature importance. GenHlth (General health score of the human that gave to
# # herself/himself) has a huge impact on the dataset and prediction scores
# but using it would be like cheating because it doesn't make sense to use an information about people that
# already knows herself/himself. In conclusion, GenHlth (General health score of the human that gave to
# herself/himself) has been dropped by sacrificing the %3 more accuracy. The algorithm runs with %71.8
# accuracy and %73.8 recall scores now with just 5 attributes. if we use all the 22 attributes (21 without
# the class) the accuracy and f1 scores would be %75. So this decision had to be made to make the application
# more user-friendly and easy to use.

data.shape

"""# Retesting the best model with these attributes"""
print()
print("Retesting the best model with these attributes")
cols = list(data.columns)
cols.remove("Diabetes_binary")
sampled, target = SMOTE().fit_resample(data[cols], data["Diabetes_binary"])
X_train, X_test, Y_train, Y_test = train_test_split(sampled[cols],
                                                    target,
                                                    test_size=0.1,
                                                    shuffle=True)

print("Train Feature Size : ", len(X_train))
print("Train Label Size : ", len(Y_train))
print("Test Feature Size : ", len(X_test))
print("Test Label Size : ", len(Y_test))

data.shape

logistic_model = LogisticRegression(solver='liblinear').fit(X_train, Y_train)
predicted = logistic_model.predict(X_test)
print("Train Accuracy : {:.2f} %".format(accuracy_score(logistic_model.predict(X_train), Y_train) * 100))
print("Test Accuracy : {:.2f} %".format(accuracy_score(logistic_model.predict(X_test), Y_test) * 100))

cm = confusion_matrix(Y_test, predicted)
classes = ["0", "1"]
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=classes)
fig, ax = plt.subplots(figsize=(10, 10))
plt.title("Confusion Matrix")
disp = disp.plot(ax=ax)
plt.show()

print(classification_report(predicted, Y_test))
print("ACCURACY:", accuracy_score(Y_test, predicted))
print("RECALL:", recall_score(Y_test, predicted, average="binary"))
print("end")

# save the model
with open("model.pkl", "wb") as out:
    pickle.dump(logistic_model, out)

# # load the model
# with open("model.pkl", "rb") as f:
#     loaded_model = pickle.load(f)
# predicted = loaded_model.predict(X_test)
#
# print("pickled model")
# print("Train Accuracy : {:.2f} %".format(accuracy_score(loaded_model.predict(X_train), Y_train) * 100))
# print("Test Accuracy : {:.2f} %".format(accuracy_score(loaded_model.predict(X_test), Y_test) * 100))
#
# cm = confusion_matrix(Y_test, predicted)
# classes = ["0", "1"]
# disp = ConfusionMatrixDisplay(confusion_matrix=cm,
#                               display_labels=classes)
# fig, ax = plt.subplots(figsize=(10, 10))
# plt.title("Confusion Matrix")
# disp = disp.plot(ax=ax)
# plt.show()
#
# print(classification_report(predicted, Y_test))
# print("ACCURACY:", 100 * accuracy_score(Y_test, predicted))
# print("RECALL:", 100 * recall_score(Y_test, predicted, average="binary"))
# print()
# print(data.shape)
# print(data.describe())
# print(data)

# # ser=pd.DataFrame(Y_test)
# index = Y_test.index
# a_list = list(index)
# print(a_list)
# for i in range(6974):
#     if predicted[i] != Y_test[i]:
#         print(a_list[i])
#
